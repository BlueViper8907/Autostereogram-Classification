{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "<font size=3rem>\n",
    "    \n",
    "0 -**[ GPU SETTINGS](#GPU-SETTINGS)**<br>\n",
    "1 -**[ BUSINESS UNDERSTANDING](#BUSINESS-UNDERSTANDING)**<br>\n",
    "2 -**[ DATA UNDERSTANDING](#DATA-UNDERSTANDING)**<br>\n",
    "3 -**[ DATA PREPARATION](#DATA-PREPARATION)**<br>\n",
    "4 -**[ MODELING](#MODELING)**<br>\n",
    "5 -**[ EVALUATION](#EVALUATION)<br>**\n",
    "</font>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Student: Cody Freese\n",
    "- Pace: Self Paced\n",
    "- Jeff Herman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Determine the difference between optical illusions and reality based images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing import image\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.applications.densenet import DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_original_start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /device:CPU:0 || Unnamed device || CPU || 256.0 MiB\n",
      " /device:GPU:0 ||  NVIDIA GeForce RTX 2070 SUPER || GPU || 5.8 GiB\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/jeffheaton/present/blob/master/youtube/gpu/keras-dual-gpu.ipynb\n",
    "# https://www.youtube.com/watch?v=HCLmM1PyDIs\n",
    "\n",
    "# List of Local Devices\n",
    "devices = device_lib.list_local_devices()\n",
    "\n",
    "# Calculate size of device\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# Data in Devices, Type, Name, Abrv, Memory\n",
    "for d in devices:\n",
    "    t = d.device_type\n",
    "    name = d.physical_device_desc\n",
    "    l = [item.split(':',1) for item in name.split(\", \")]\n",
    "    name_attr = dict([x for x in l if len(x)==2])\n",
    "    dev = name_attr.get('name', 'Unnamed device')\n",
    "    print(f\" {d.name} || {dev} || {t} || {sizeof_fmt(d.memory_limit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU's Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of GPU's Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pixels(pixel_path):\n",
    "    '''\n",
    "    Gets width and height of image\n",
    "    :param pixel_path: String\n",
    "    :return width, height: Tuple of Int\n",
    "    '''\n",
    "    \n",
    "    width, height = Image.open(pixel_path).size\n",
    "    return width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pixels_and_paths(path):\n",
    "    '''\n",
    "    Joins the classes of called picture pixel count for total of called class\n",
    "    :param path: String\n",
    "    :return None: prints out pixel dimensions\n",
    "    '''\n",
    "    \n",
    "    # Iterate through 5 entries to print number of pixels from image\n",
    "    for path, subdirs, files in os.walk(path):\n",
    "        for index, name in enumerate(files):\n",
    "            image_path = os.path.join(path,name)\n",
    "            print(get_num_pixels(image_path))\n",
    "            if index >= 4:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Sourced from Learn.Co, modified for this project\n",
    "# https://github.com/learn-co-curriculum/dsc-image-classification-with-mlps-lab/tree/solution\n",
    "def visualize_training_results(results):\n",
    "    '''\n",
    "    Renders out metric results into graphs\n",
    "    :param results: tensorflow.python.keras.callbacks.History\n",
    "    :return None: print out graphs\n",
    "    '''\n",
    "    # Assign variable for model history results\n",
    "    history = results.history\n",
    "    \n",
    "    # Loss metric\n",
    "    plt.figure()\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.legend(['loss','val_loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy metric\n",
    "    plt.figure()\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.legend(['accuracy','val_accuracy'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision metric\n",
    "    plt.figure()\n",
    "    plt.plot(history['precision'])\n",
    "    plt.plot(history['val_precision'])\n",
    "    plt.legend(['precision','val_precision'])\n",
    "    plt.title('Precision')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "    \n",
    "    # Recall metric\n",
    "    plt.figure()\n",
    "    plt.plot(history['recall'])\n",
    "    plt.plot(history['val_recall'])\n",
    "    plt.legend(['recall','val_recall'])\n",
    "    plt.title('Recall')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Home Directory\n",
    "home_path = r'C:\\Users\\c_fre\\Learn.co\\Module_5_Project\\Illusion-Image-Classification\\illusion_images'\n",
    "\n",
    "#Train Directory\n",
    "train_path = os.path.join(home_path,'train')\n",
    "\n",
    "#Validation Directory\n",
    "val_path = os.path.join(home_path,'val')\n",
    "\n",
    "#Test Directory\n",
    "test_path = os.path.join(home_path,'test')\n",
    "\n",
    "#Train Subdirectories\n",
    "normal_train_path = os.path.join(train_path,'NORMAL')\n",
    "\n",
    "pneumonia_train_path = os.path.join(train_path,'ILLUSION')\n",
    "\n",
    "#Validation Subdirectories\n",
    "normal_val_path = os.path.join(val_path,'NORMAL')\n",
    "\n",
    "pneumonia_val_path = os.path.join(val_path,'ILLUSION')\n",
    "\n",
    "#Test Subdirectories\n",
    "\n",
    "normal_test_path = os.path.join(test_path,'NORMAL')\n",
    "\n",
    "pneumonia_test_path = os.path.join(test_path,'ILLUSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print to Verify Directory Pathways\n",
    "print('Directory Contents:',os.listdir(home_path))\n",
    "print('train Contents:',os.listdir(train_path))\n",
    "print('val Contents:',os.listdir(val_path))\n",
    "print('test Contents:',os.listdir(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cases in Training Sets\n",
    "print(\"Number of cases in Training:\",len(os.listdir(normal_train_path)) +len(os.listdir(pneumonia_train_path)))\n",
    "\n",
    "#Pneumonia Cases in Training\n",
    "print(\"Number of Pneumonia cases in Training:\",len(os.listdir(pneumonia_train_path)))\n",
    "\n",
    "#Normal Cases in Training\n",
    "print(\"Number of Normal cases in Training:\",len(os.listdir(normal_train_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cases in Validation Sets\n",
    "print(\"Number of cases in Validation:\",len(os.listdir(normal_val_path)) +len(os.listdir(pneumonia_val_path)))\n",
    "\n",
    "# Pneumonia Cases in Validation\n",
    "print(\"Number of Pneumonia cases in Validation:\",len(os.listdir(pneumonia_val_path)))\n",
    "\n",
    "# Normal Cases in Validation\n",
    "print(\"Number of Normal cases in Validation:\",len(os.listdir(normal_val_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cases in Test Sets\n",
    "print(\"Number of cases in Test:\",len(os.listdir(normal_test_path)) +len(os.listdir(pneumonia_test_path)))\n",
    "\n",
    "#Pneumonia Cases in Test\n",
    "print(\"Number of Pneumonia cases in Test:\",len(os.listdir(pneumonia_test_path)))\n",
    "\n",
    "#Normal cases in Test\n",
    "print(\"Number of Normal cases in Test:\",len(os.listdir(normal_test_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Total number of Normal Cases\n",
    "print(\"Total number of Normal cases:\",\n",
    "      len(os.listdir(normal_test_path))+\n",
    "      len(os.listdir(normal_train_path))+\n",
    "      len(os.listdir(normal_val_path)))\n",
    "\n",
    "#Total number of Pneumonia Cases\n",
    "print(\"Total number of Pneumonia cases:\",\n",
    "      len(os.listdir(pneumonia_test_path))+\n",
    "      len(os.listdir(pneumonia_train_path))+\n",
    "      len(os.listdir(pneumonia_val_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Pixel Size\n",
    "get_image_pixels_and_paths(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all images in Normal Validatoin pathway\n",
    "normal_val_images_display = os.listdir(normal_val_path)[:5]\n",
    "for image_name in normal_val_images_display:\n",
    "    image_path = os.path.join(normal_val_path, image_name)\n",
    "    img = image.load_img(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all images in Pneumonia validation pathway\n",
    "pneumonia_val_image_display = os.listdir(pneumonia_val_path)[:5]\n",
    "for image_name in pneumonia_val_image_display:\n",
    "    image_path = os.path.join(pneumonia_val_path, image_name)\n",
    "    img = image.load_img(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double default batch size due to computer hardware\n",
    "batch_size = 64\n",
    "\n",
    "# Set all image sizes universally\n",
    "img_width, img_height = 200,200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation of Training data, rescale\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1 / 255.0,\n",
    "        rotation_range=20,\n",
    "        zoom_range=0.05,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        shear_range=0.05,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\")\n",
    "\n",
    "\n",
    "# Rescale validation data\n",
    "val_datagen = ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "# Rescale test data\n",
    "test_datagen = ImageDataGenerator(rescale=1 / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(img_width, img_height),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True,\n",
    "    seed=7539541\n",
    ")\n",
    "\n",
    "# Validation Generator\n",
    "valid_generator = val_datagen.flow_from_directory(\n",
    "    directory=val_path,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True,\n",
    "    seed=7539541\n",
    ")\n",
    "\n",
    "# Test Generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(img_width, img_height),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True,\n",
    "    seed=7539541\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time of model\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential: as we want to build layers upon each other\n",
    "mlp_model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse paths without losing scalability\n",
    "mlp_model.add(keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense: Fully Connected Layer, Reduce Gradient Vanish, Stop Inactive Neurons with relu\n",
    "mlp_model.add(keras.layers.Dense(64,\n",
    "                                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense: Fully Connected Layer, Reduce Gradient Vanish, Stop Inactive Neurons with relu\n",
    "mlp_model.add(keras.layers.Dense(32,\n",
    "                                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense: Fully Connected Layer, Reduce Gradient Vanish, Stop Inactive Neurons with relu\n",
    "mlp_model.add(keras.layers.Dense(16,\n",
    "                                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense: Fully Connected Layer, Reduce Gradient Vanish, Stop Inactive Neurons with relu\n",
    "mlp_model.add(keras.layers.Dense(8,\n",
    "                                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full input, Binary output with sigmoid\n",
    "mlp_model.add(keras.layers.Dense(1,\n",
    "                                 activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile. Binary Crossentropy to Predict Probability to actual class output, Stochastic Gradient Descent via Adam\n",
    "mlp_model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy',\n",
    "                           tf.keras.metrics.Precision(name='precision'),\n",
    "                           tf.keras.metrics.Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model, 10 Epochs\n",
    "mlp_history = mlp_model.fit(train_generator,\n",
    "                            validation_data = valid_generator,\n",
    "                            steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
    "                            validation_steps = valid_generator.n//valid_generator.batch_size,\n",
    "                            epochs=10)\n",
    "\n",
    "# Stop time of model\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model Locally\n",
    "mlp_model.save(\"MLP_Initial_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Model\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Metrics\n",
    "visualize_training_results(mlp_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against Training set\n",
    "mlp_results_train = mlp_model.evaluate(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against Test set\n",
    "mlp_results_test = mlp_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time of model\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential: as we want to build layers upon each other\n",
    "cnn_model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution Layer over Images, Generate 64 filters and multiply across the image\n",
    "# Filter Images to highlight features\n",
    "cnn_model.add(keras.layers.Conv2D(64,\n",
    "                                  kernel_size=(3,3),\n",
    "                                  activation='relu',\n",
    "                                  input_shape=(img_width, img_height, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group images into 2x2, votes on largest, features maintained\n",
    "cnn_model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse paths without losing scalability\n",
    "cnn_model.add(keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense: Fully Connected Layer, Reduce Gradient Vanish, Stop Inactive Neurons with relu\n",
    "cnn_model.add(keras.layers.Dense(64,\n",
    "                                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense: Fully Connected Layer, Reduce Gradient Vanish, Stop Inactive Neurons with relu\n",
    "cnn_model.add(keras.layers.Dense(32,\n",
    "                                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense: Fully Connected Layer, Reduce Gradient Vanish, Stop Inactive Neurons with relu\n",
    "cnn_model.add(keras.layers.Dense(16,\n",
    "                                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense: Fully Connected Layer, Reduce Gradient Vanish, Stop Inactive Neurons with relu\n",
    "cnn_model.add(keras.layers.Dense(8,\n",
    "                                 activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full input, Binary output with sigmoid\n",
    "cnn_model.add(keras.layers.Dense(1,\n",
    "                                 activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile. Binary Crossentropy to Predict Probability to actual class output, Stochastic Gradient Descent via Adam\n",
    "cnn_model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy',\n",
    "                           tf.keras.metrics.Precision(name='precision'),\n",
    "                           tf.keras.metrics.Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Model\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model, 10 Epochs\n",
    "cnn_history = cnn_model.fit(train_generator,\n",
    "                        validation_data = valid_generator,\n",
    "                        steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
    "                        validation_steps = valid_generator.n//valid_generator.batch_size,\n",
    "                        epochs=10)\n",
    "\n",
    "# Stop time of model\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "cnn_model.save(\"CNN_Initial_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Metrics\n",
    "visualize_training_results(cnn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against Training set\n",
    "cnn_results_train = cnn_model.evaluate(train_generator)\n",
    "cnn_results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against Test set\n",
    "cnn_results_test = cnn_model.evaluate(test_generator)\n",
    "cnn_results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time of model\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint, Monitor Loss, Save best results\n",
    "mlp_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"MLP_Finetune_Checkpoint\",\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                save_best_only=True)\n",
    "\n",
    "# Early stop after 10 efforts to learn, each Epoch starts with best saved weights\n",
    "mlp_early_stopping = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  monitor=\"val_loss\",\n",
    "                                                  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model, 100 Epochs, Checkpoint and Early Stop\n",
    "mlp_finetune_history = mlp_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "    epochs=100,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps = valid_generator.n//valid_generator.batch_size,\n",
    "    callbacks=[mlp_checkpoint, mlp_early_stopping]\n",
    ")\n",
    "\n",
    "# Stop time of model\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Metrics\n",
    "visualize_training_results(mlp_finetune_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model against Training set\n",
    "mlp_results_train = mlp_model.evaluate(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model again Test set\n",
    "mlp_results_test = mlp_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time of model\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint, Monitor Loss, Save best results\n",
    "cnn_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"CNN_Finetune_Checkpoint\",\n",
    "                                                monitor=\"loss\",\n",
    "                                                save_best_only=True)\n",
    "\n",
    "# Early stop after 10 efforts to learn, each Epoch starts with best saved weights\n",
    "cnn_early_stopping = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  monitor=\"loss\",\n",
    "                                                  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model, 100 Epochs, Checkpoint and Early Stop\n",
    "cnn_finetune_history = cnn_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "    epochs=100,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps = valid_generator.n//valid_generator.batch_size,\n",
    "    callbacks=[cnn_checkpoint, cnn_early_stopping]\n",
    ")\n",
    "\n",
    "# Stop time of model\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Metrics\n",
    "visualize_training_results(cnn_finetune_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against Training set\n",
    "cnn_results_train = cnn_model.evaluate(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against Test set\n",
    "cnn_results_test = cnn_model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time of model\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pretrain Base Model\n",
    "pretrain_base = DenseNet201(weights=None,\n",
    "                 include_top=False,\n",
    "                 input_shape=(200, 200, 3))\n",
    "\n",
    "# Define Model Architecture\n",
    "pretrain_model = keras.Sequential()\n",
    "pretrain_model.add(pretrain_base)\n",
    "pretrain_model.add(keras.layers.Flatten())\n",
    "pretrain_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "pretrain_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "pretrain_model.add(keras.layers.Dense(256, activation='relu'))\n",
    "pretrain_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "pretrain_model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Do NOT train the base\n",
    "pretrain_base.trainable = False\n",
    "\n",
    "# Summary of Model\n",
    "pretrain_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile. Binary Crossentropy to Predict Probability to actual class output, Stochastic Gradient Descent via Adam\n",
    "pretrain_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy',\n",
    "                      tf.keras.metrics.Precision(name='precision'),\n",
    "                      tf.keras.metrics.Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model, 10 Epochs\n",
    "pretrain_model_history = pretrain_model.fit(train_generator,\n",
    "                                            validation_data = valid_generator,\n",
    "                                            steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
    "                                            validation_steps = valid_generator.n//valid_generator.batch_size,\n",
    "                                            epochs=10)\n",
    "\n",
    "# Stop time of model\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "pretrain_model.save(\"Pretrain_Initial_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Metrics\n",
    "visualize_training_results(pretrain_model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model against Training set\n",
    "pretrain_results_train = pretrain_model.evaluate(train_generator)\n",
    "pretrain_results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model against Test set\n",
    "pretrain_results_test = pretrain_model.evaluate(test_generator)\n",
    "pretrain_results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time of model\n",
    "original_start = datetime.datetime.now()\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint, Monitor Loss, Save best results\n",
    "pretrain_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"Finetune_Pretrain_Checkpoint\",\n",
    "                                                               monitor=\"loss\",\n",
    "                                                               save_best_only=True)\n",
    "\n",
    "# Early stop after 10 efforts to learn, each Epoch starts with best saved weights\n",
    "pretrain_model_early_stopping = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                                 monitor=\"loss\",\n",
    "                                                                 restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pretrain Base Model\n",
    "finetune_pretrain_base = DenseNet201(weights=None,\n",
    "                                     include_top=False,\n",
    "                                     input_shape=(200, 200, 3))\n",
    "\n",
    "# Define Model Architecture\n",
    "finetune_pretrain_base = keras.Sequential()\n",
    "finetune_pretrain_base.add(pretrain_base)\n",
    "finetune_pretrain_base.add(keras.layers.Flatten())\n",
    "finetune_pretrain_base.add(keras.layers.Dense(64, activation='relu'))\n",
    "finetune_pretrain_base.add(keras.layers.Dense(128, activation='relu'))\n",
    "finetune_pretrain_base.add(keras.layers.Dense(256, activation='relu'))\n",
    "finetune_pretrain_base.add(keras.layers.Dense(128, activation='relu'))\n",
    "finetune_pretrain_base.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Do NOT train the base\n",
    "finetune_pretrain_base.trainable = False\n",
    "\n",
    "# Summary of Model\n",
    "finetune_pretrain_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile. Binary Crossentropy to Predict Probability to actual class output, Stochastic Gradient Descent via Adam\n",
    "finetune_pretrain_base.compile(loss=\"binary_crossentropy\",\n",
    "                               optimizer=\"adam\",\n",
    "                               metrics=['accuracy',\n",
    "                                        tf.keras.metrics.Precision(name='precision'),\n",
    "                                        tf.keras.metrics.Recall(name='recall')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model, 100 Epochs, Checkpoint and Early Stop\n",
    "finetune_pretrain_model_history = pretrain_model.fit(train_generator,\n",
    "                                                     validation_data = valid_generator,\n",
    "                                                     steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
    "                                                     validation_steps = valid_generator.n//valid_generator.batch_size,\n",
    "                                                     callbacks=[pretrain_model_checkpoint, pretrain_model_early_stopping],\n",
    "                                                     epochs=100)\n",
    "\n",
    "# Stop time of model\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training took a total of {}'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Metrics\n",
    "visualize_training_results(finetune_pretrain_model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model against Training set\n",
    "finetune_pretrain_results_train = pretrain_model.evaluate(train_generator)\n",
    "finetune_pretrain_results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model against Test set\n",
    "finetune_pretrain_results_test = pretrain_model.evaluate(test_generator)\n",
    "finetune_pretrain_results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "mlp_original_model = keras.models.load_model('MLP_Initial_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = mlp_original_model.evaluate(test_generator)\n",
    "print(\"Loss of the model: %.2f%%\"%(scores[0] * 100))\n",
    "print(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))\n",
    "print(\"Test Precision: %.2f%%\"%(scores[2] * 100))\n",
    "print(\"Test Recall: %.2f%%\"%(scores[3] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "mlp_finetune_model = keras.models.load_model('MLP_Finetune_Checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized, Organized, Evaluate against Test set with metrics\n",
    "scores = mlp_finetune_model.evaluate(test_generator)\n",
    "print(\"Loss of the model: %.2f%%\"%(scores[0] * 100))\n",
    "print(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))\n",
    "print(\"Test Precision: %.2f%%\"%(scores[2] * 100))\n",
    "print(\"Test Recall: %.2f%%\"%(scores[3] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "cnn_initial_model = keras.models.load_model('CNN_Initial_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized, Organized, Evaluate against Test set with metrics\n",
    "scores = cnn_initial_model.evaluate(test_generator)\n",
    "print(\"Loss of the model: %.2f%%\"%(scores[0] * 100))\n",
    "print(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))\n",
    "print(\"Test Precision: %.2f%%\"%(scores[2] * 100))\n",
    "print(\"Test Recall: %.2f%%\"%(scores[3] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "cnn_finetune_model = keras.models.load_model('CNN_Finetune_Checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized, Organized, Evaluate against Test set with metrics\n",
    "scores = cnn_finetune_model.evaluate(test_generator)\n",
    "print(\"Loss of the model: %.2f%%\"%(scores[0] * 100))\n",
    "print(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))\n",
    "print(\"Test Precision: %.2f%%\"%(scores[2] * 100))\n",
    "print(\"Test Recall: %.2f%%\"%(scores[3] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "pretrain_model = keras.models.load_model('Pretrain_Initial_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized, Organized, Evaluate against Test set with metrics\n",
    "scores = pretrain_model.evaluate(test_generator)\n",
    "print(\"Loss of the model: %.2f%%\"%(scores[0] * 100))\n",
    "print(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))\n",
    "print(\"Test Precision: %.2f%%\"%(scores[2] * 100))\n",
    "print(\"Test Recall: %.2f%%\"%(scores[3] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "pretrain_finetune_model = keras.models.load_model('Finetune_Pretrain_Checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalized, Organized, Evaluate against Test set with metrics\n",
    "scores = pretrain_finetune_model.evaluate(test_generator)\n",
    "print(\"Loss of the model: %.2f%%\"%(scores[0] * 100))\n",
    "print(\"Test Accuracy: %.2f%%\"%(scores[1] * 100))\n",
    "print(\"Test Precision: %.2f%%\"%(scores[2] * 100))\n",
    "print(\"Test Recall: %.2f%%\"%(scores[3] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Stop, Total Time.\n",
    "notebook_end = datetime.datetime.now()\n",
    "notebook_elapsed = notebook_end - notebook_original_start\n",
    "print('Notebook took a total of {}'.format(elapsed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
